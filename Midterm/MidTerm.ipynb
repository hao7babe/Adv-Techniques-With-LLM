{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4611c0e8",
   "metadata": {},
   "source": [
    "# Q1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "740535c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent Iterations for Learning Rate 0.4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration</th>\n",
       "      <th>x_old</th>\n",
       "      <th>x_new</th>\n",
       "      <th>Gradient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.60</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Iteration  x_old  x_new  Gradient\n",
       "0          1    4.0   1.60       6.0\n",
       "1          2    1.6   1.12       1.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient Descent Iterations for Learning Rate 0.7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration</th>\n",
       "      <th>x_old</th>\n",
       "      <th>x_new</th>\n",
       "      <th>Gradient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4.00</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>1.480</td>\n",
       "      <td>-2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Iteration  x_old  x_new  Gradient\n",
       "0          1   4.00 -0.200      6.00\n",
       "1          2  -0.20  1.480     -2.40\n",
       "2          3   1.48  0.808      0.96"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the gradient descent function\n",
    "def gradient_descent(start_x, learning_rate, tolerance=0.01, max_iterations=100):\n",
    "    x = start_x\n",
    "    iterations = []\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        # Calculate gradient f'(x) = 2x - 2\n",
    "        gradient = 2 * x - 2\n",
    "        # Update x\n",
    "        x_new = x - learning_rate * gradient\n",
    "        \n",
    "        # Store iteration details\n",
    "        iterations.append((i + 1, x, x_new, gradient))\n",
    "        \n",
    "        # Check if x is within the desired range (0.8, 1.2)\n",
    "        if 0.8 <= x_new <= 1.2:\n",
    "            break\n",
    "        \n",
    "        # Update x\n",
    "        x = x_new\n",
    "    \n",
    "    return iterations\n",
    "\n",
    "# Initial parameters for both cases\n",
    "start_x = 4\n",
    "learning_rate_A = 0.4\n",
    "learning_rate_B = 0.7\n",
    "\n",
    "# Run gradient descent for both cases\n",
    "iterations_A = gradient_descent(start_x, learning_rate_A)\n",
    "iterations_B = gradient_descent(start_x, learning_rate_B)\n",
    "\n",
    "# Creating DataFrames to display iterations for each case\n",
    "df_A = pd.DataFrame(iterations_A, columns=[\"Iteration\", \"x_old\", \"x_new\", \"Gradient\"])\n",
    "df_B = pd.DataFrame(iterations_B, columns=[\"Iteration\", \"x_old\", \"x_new\", \"Gradient\"])\n",
    "\n",
    "# Display the results\n",
    "print(\"Gradient Descent Iterations for Learning Rate 0.4\")\n",
    "display(df_A)\n",
    "\n",
    "print(\"\\nGradient Descent Iterations for Learning Rate 0.7\")\n",
    "display(df_B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d77076a",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73cf189a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H11: 17.6\n",
      "H12: 3.1\n",
      "H21: 12.440000000000001\n",
      "H22: 9.090000000000002\n",
      "O1 (Sigmoid Output): 0.15775868701896995\n",
      "Predicted Class: No_Heart_Attack (0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Input values\n",
    "I1 = 10  # Blood Pressure\n",
    "I2 = 20  # BMI\n",
    "\n",
    "# Weights and biases for hidden layer 1\n",
    "W_H11 = [0.2, 0.8]\n",
    "b_H11 = -0.4\n",
    "\n",
    "W_H12 = [0.1, 0.1]\n",
    "b_H12 = 0.1\n",
    "\n",
    "# Hidden layer 1 computations\n",
    "H11 = relu(W_H11[0] * I1 + W_H11[1] * I2 + b_H11)\n",
    "H12 = relu(W_H12[0] * I1 + W_H12[1] * I2 + b_H12)\n",
    "\n",
    "print(f\"H11: {H11}\")\n",
    "print(f\"H12: {H12}\")\n",
    "\n",
    "# Weights and biases for hidden layer 2\n",
    "W_H21 = [0.7, 0.2]\n",
    "b_H21 = -0.5\n",
    "\n",
    "W_H22 = [0.4, 0.5]\n",
    "b_H22 = 0.5\n",
    "\n",
    "# Hidden layer 2 computations\n",
    "H21 = relu(W_H21[0] * H11 + W_H21[1] * H12 + b_H21)\n",
    "H22 = relu(W_H22[0] * H11 + W_H22[1] * H12 + b_H22)\n",
    "\n",
    "print(f\"H21: {H21}\")\n",
    "print(f\"H22: {H22}\")\n",
    "\n",
    "# Weights and biases for output layer\n",
    "W_O1 = [-0.5, 0.5]\n",
    "\n",
    "# Output layer computation\n",
    "O1 = sigmoid(W_O1[0] * H21 + W_O1[1] * H22)\n",
    "\n",
    "print(f\"O1 (Sigmoid Output): {O1}\")\n",
    "\n",
    "# Classification based on threshold\n",
    "threshold = 0.5\n",
    "prediction = 1 if O1 >= threshold else 0\n",
    "\n",
    "print(f\"Predicted Class: {'Heart_Attack (1)' if prediction == 1 else 'No_Heart_Attack (0)'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb440008",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98385736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "  Most Frequent Pair: ('l', 'o') (Frequency: 2)\n",
      "  Vocabulary State: ['low</w>', 'lower</w>', 'high</w>', 'higher</w>']\n",
      "\n",
      "Iteration 2:\n",
      "  Most Frequent Pair: ('lo', 'w') (Frequency: 2)\n",
      "  Vocabulary State: ['low</w>', 'lower</w>', 'high</w>', 'higher</w>']\n",
      "\n",
      "Iteration 3:\n",
      "  Most Frequent Pair: ('e', 'r') (Frequency: 2)\n",
      "  Vocabulary State: ['low</w>', 'lower</w>', 'high</w>', 'higher</w>']\n",
      "\n",
      "Iteration 4:\n",
      "  Most Frequent Pair: ('er', '</w>') (Frequency: 2)\n",
      "  Vocabulary State: ['low</w>', 'lower</w>', 'high</w>', 'higher</w>']\n",
      "\n",
      "Iteration 5:\n",
      "  Most Frequent Pair: ('h', 'i') (Frequency: 2)\n",
      "  Vocabulary State: ['low</w>', 'lower</w>', 'high</w>', 'higher</w>']\n",
      "\n",
      "Iteration 6:\n",
      "  Most Frequent Pair: ('hi', 'g') (Frequency: 2)\n",
      "  Vocabulary State: ['low</w>', 'lower</w>', 'high</w>', 'higher</w>']\n",
      "\n",
      "Iteration 7:\n",
      "  Most Frequent Pair: ('hig', 'h') (Frequency: 2)\n",
      "  Vocabulary State: ['low</w>', 'lower</w>', 'high</w>', 'higher</w>']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Initial training data\n",
    "training_data = [\"low\", \"lower\", \"high\", \"higher\"]\n",
    "\n",
    "# Preprocess training data to tokenize words as lists of characters with a special end-of-word token\n",
    "def preprocess_data(data):\n",
    "    return [list(word) + ['</w>'] for word in data]\n",
    "\n",
    "# Count pairs in a given vocabulary\n",
    "def get_pair_frequencies(vocab):\n",
    "    pairs = defaultdict(int)\n",
    "    for word in vocab:\n",
    "        for i in range(len(word) - 1):\n",
    "            pairs[(word[i], word[i + 1])] += 1\n",
    "    return pairs\n",
    "\n",
    "# Merge the most frequent pair in the vocabulary\n",
    "def merge_pair(vocab, pair):\n",
    "    new_vocab = []\n",
    "    bigram = \"\".join(pair)\n",
    "    for word in vocab:\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            if i < len(word) - 1 and (word[i], word[i + 1]) == pair:\n",
    "                new_word.append(bigram)  # Merge pair\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        new_vocab.append(new_word)\n",
    "    return new_vocab\n",
    "\n",
    "# Main BPE algorithm with a min-frequency threshold\n",
    "def byte_pair_encoding(data, min_frequency=2):\n",
    "    vocab = preprocess_data(data)\n",
    "    iterations = []\n",
    "    \n",
    "    while True:\n",
    "        # Get frequency of each pair\n",
    "        pair_freq = get_pair_frequencies(vocab)\n",
    "        \n",
    "        # Find the most common pair with frequency >= min_frequency\n",
    "        most_frequent_pair = None\n",
    "        max_freq = 0\n",
    "        for pair, freq in pair_freq.items():\n",
    "            if freq > max_freq and freq >= min_frequency:\n",
    "                most_frequent_pair = pair\n",
    "                max_freq = freq\n",
    "        \n",
    "        # If no pairs meet the frequency requirement, break\n",
    "        if most_frequent_pair is None:\n",
    "            break\n",
    "        \n",
    "        # Merge the most frequent pair\n",
    "        vocab = merge_pair(vocab, most_frequent_pair)\n",
    "        \n",
    "        # Save the iteration state\n",
    "        iterations.append({\n",
    "            \"pair\": most_frequent_pair,\n",
    "            \"frequency\": max_freq,\n",
    "            \"vocab_state\": [\"\".join(word) for word in vocab]\n",
    "        })\n",
    "    \n",
    "    return iterations\n",
    "\n",
    "# Run BPE on the training data with min frequency 2\n",
    "bpe_iterations = byte_pair_encoding(training_data, min_frequency=2)\n",
    "\n",
    "# Display the iterations\n",
    "for i, iteration in enumerate(bpe_iterations):\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    print(f\"  Most Frequent Pair: {iteration['pair']} (Frequency: {iteration['frequency']})\")\n",
    "    print(f\"  Vocabulary State: {iteration['vocab_state']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5315bbe",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2fc802",
   "metadata": {},
   "source": [
    "Suppose that word embeddings (with the same dimensions) were created using an approach which preserves relationships between embeddings precisely.\n",
    "\n",
    "A) If the following equations are correct calculate the embedding of “brothers”:\n",
    "\n",
    "E(students) - E(student) = [0,0,2]\n",
    "\n",
    "E(father) - E(mother) = [0,1,0]\n",
    "\n",
    "E(sister)=[-2,1,0]\n",
    "\n",
    " \n",
    "\n",
    "E(brothers)= ?\n",
    "\n",
    " \n",
    "\n",
    "B) Suppose that the following sentence is processed using self-attention mechanism and the context is already added to embeddings of words. What is the embedding of “sibling”  after adding the context by the self-attention? \n",
    "\n",
    "Sentence: “I have one male sibling”\n",
    "\n",
    " \n",
    "\n",
    "E(sibling)= ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7a7f78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding of brothers (E(brothers)): [0 1 2]\n",
      "Embedding of sibling with context (E(sibling)): [1 1 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given information:\n",
    "E_students_minus_E_student = np.array([0, 0, 2])\n",
    "E_father_minus_E_mother = np.array([0, 1, 0])\n",
    "E_sister = np.array([-2, 1, 0])\n",
    "\n",
    "# Part A: Calculating E(brothers)\n",
    "# Assuming E(brothers) can be calculated similarly to the plural relationship:\n",
    "# E(brothers) = E(father) + (E(students) - E(student))\n",
    "# Since we don't have E(father) directly, but we have E(father) - E(mother), \n",
    "# we'll need to express E(brothers) in terms of E(mother).\n",
    "\n",
    "# Let E_mother be the base, then:\n",
    "# E_father = E_mother + E(father - mother)\n",
    "# E_brothers = E_father + (E(students) - E(student))\n",
    "# Substituting E_father:\n",
    "E_mother = np.array([0, 0, 0])  # We'll assume E_mother as [0,0,0] if no explicit embedding is given\n",
    "E_father = E_mother + E_father_minus_E_mother\n",
    "E_brothers = E_father + E_students_minus_E_student\n",
    "\n",
    "print(\"Embedding of brothers (E(brothers)):\", E_brothers)\n",
    "\n",
    "# Part B: Embedding of \"sibling\" after adding context by self-attention\n",
    "# Assuming the context added for the word \"male\" to \"sibling\" aligns sibling towards male attributes.\n",
    "# Since we lack specific contextual vectors, let's assume sibling's embedding is similar to an average of male-related embeddings\n",
    "# in this simplified context. \n",
    "\n",
    "# Hypothetically, let's say the context embedding shift for \"male sibling\" is [1, 0, 0] to indicate gender aspect.\n",
    "# E(sibling) after self-attention would be:\n",
    "E_sibling = E_brothers + np.array([1, 0, 0])  # Adding context indicating \"male\"\n",
    "\n",
    "print(\"Embedding of sibling with context (E(sibling)):\", E_sibling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4966b51",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1929bf44",
   "metadata": {},
   "source": [
    "**Use Greedy and Beam Search (beam width=3) as decoding strategies for a sentence which starts with “I” and the following probabilities. Show steps of both decoding strategies and the resulting sentences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b9748cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Search Result:\n",
      "Sentence: I buy it\n",
      "\n",
      "Beam Search Results (Top 3):\n",
      "Sentence: I have it, Probability: 0.04000000000000001\n",
      "Sentence: I have that, Probability: 0.03\n",
      "Sentence: I had it, Probability: 0.03\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the probability dictionary for each word and its possible next words\n",
    "probabilities = {\n",
    "    \"I\": {\n",
    "        \"have\": 0.20,\n",
    "        \"buy\": 0.25,\n",
    "        \"had\": 0.15,\n",
    "        \"got\": 0.10\n",
    "    },\n",
    "    \"have\": {\n",
    "        \"this\": 0.10,\n",
    "        \"that\": 0.15,\n",
    "        \"it\": 0.20,\n",
    "        \"not\": 0.05\n",
    "    },\n",
    "    \"buy\": {\n",
    "        \"it\": 0.10,\n",
    "        \"nothing\": 0.05,\n",
    "        \"anything\": 0.01,\n",
    "        \"something\": 0.05\n",
    "    },\n",
    "    \"had\": {\n",
    "        \"this\": 0.10,\n",
    "        \"that\": 0.10,\n",
    "        \"it\": 0.20,\n",
    "        \"not\": 0.02\n",
    "    },\n",
    "    \"got\": {\n",
    "        \"nothing\": 0.05,\n",
    "        \"it\": 0.20,\n",
    "        \"something\": 0.10,\n",
    "        \"this\": 0.10\n",
    "    }\n",
    "}\n",
    "\n",
    "# Greedy Search\n",
    "def greedy_search(start_word, probabilities):\n",
    "    sentence = [start_word]\n",
    "    current_word = start_word\n",
    "    \n",
    "    while current_word in probabilities:\n",
    "        # Select the next word with the highest probability\n",
    "        next_word = max(probabilities[current_word], key=probabilities[current_word].get)\n",
    "        sentence.append(next_word)\n",
    "        current_word = next_word\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "# Beam Search\n",
    "def beam_search(start_word, probabilities, beam_width=3):\n",
    "    beams = [(start_word, 1.0)]  # Each beam is a tuple (sentence, probability)\n",
    "    \n",
    "    for _ in range(2):  # Limit to two steps (Step1 and Step2) for this example\n",
    "        new_beams = []\n",
    "        \n",
    "        for sentence, prob in beams:\n",
    "            last_word = sentence.split()[-1]\n",
    "            \n",
    "            if last_word in probabilities:\n",
    "                # Expand each beam by the possible next words\n",
    "                for next_word, next_prob in probabilities[last_word].items():\n",
    "                    new_sentence = sentence + \" \" + next_word\n",
    "                    new_prob = prob * next_prob\n",
    "                    new_beams.append((new_sentence, new_prob))\n",
    "        \n",
    "        # Sort beams by probability and keep the top-k (beam width)\n",
    "        new_beams = sorted(new_beams, key=lambda x: x[1], reverse=True)\n",
    "        beams = new_beams[:beam_width]\n",
    "    \n",
    "    return beams\n",
    "\n",
    "# Perform Greedy Search and Beam Search\n",
    "greedy_result = greedy_search(\"I\", probabilities)\n",
    "beam_result = beam_search(\"I\", probabilities, beam_width=3)\n",
    "\n",
    "# Display Results\n",
    "print(\"Greedy Search Result:\")\n",
    "print(\"Sentence:\", \" \".join(greedy_result))\n",
    "print(\"\\nBeam Search Results (Top 3):\")\n",
    "for sentence, prob in beam_result:\n",
    "    print(f\"Sentence: {sentence}, Probability: {prob}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2ae03a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
